{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jsonlines) (1.11.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-1.2.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv, jsonlines\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "# Importing tensorflow\n",
    "import tensorflow as tf\n",
    "# Importing some more libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading ratings file\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', \n",
    "                      usecols=['user_id', 'movie_id', 'user_emb_id', 'movie_emb_id', 'rating'])\n",
    "max_userid = ratings['user_id'].drop_duplicates().max()\n",
    "max_movieid = ratings['movie_id'].drop_duplicates().max()\n",
    "\n",
    "# Reading ratings file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', \n",
    "                    usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading ratings file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', \n",
    "                     usecols=['movie_id', 'title', 'genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need an interaction matrix of ratings where column and row align to user_id and movie_id\n",
    "ratings = ratings.drop('user_emb_id', axis=1).drop('movie_emb_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USERS: 6040 MOVIES: 3706\n"
     ]
    }
   ],
   "source": [
    "num_movies = ratings.movie_id.nunique()\n",
    "num_users = ratings.user_id.nunique()\n",
    "print(\"USERS: {} MOVIES: {}\".format(num_users, num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\n",
      "\n",
      "Users: 6,040\n",
      "Items: 3,706\n",
      "Ratings: 1,000,209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "import os\n",
    "\n",
    "Rating = collections.namedtuple('Rating', ['user_id', 'item_id', 'rating'])\n",
    "\n",
    "class Dataset(collections.namedtuple('Dataset', ['users', 'items', 'ratings'])):\n",
    "\n",
    "    #users: set[str]\n",
    "    #items: set[str]\n",
    "    #ratings: list[Rating]\n",
    "\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __str__(self):\n",
    "        out = 'Users: {:,d}\\n'.format(self.n_users)\n",
    "        out += 'Items: {:,d}\\n'.format(self.n_items)\n",
    "        out += 'Ratings: {:,d}\\n'.format(self.n_ratings)\n",
    "        return out\n",
    "    \n",
    "    @property\n",
    "    def n_users(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    @property\n",
    "    def n_items(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    @property\n",
    "    def n_ratings(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def user_ratings(self, user_id):\n",
    "        return list(r for r in self.ratings if r.user_id == user_id)\n",
    "\n",
    "    def item_ratings(self, item_id):\n",
    "        return list(r for r in self.ratings if r.item_id == item_id)\n",
    "\n",
    "    def filter_ratings(self, users, items):\n",
    "        return list(((r.user_id, r.item_id), r.rating)\n",
    "                    for r in self.ratings\n",
    "                    if r.user_id in users\n",
    "                    and r.item_id in items)\n",
    "\n",
    "\n",
    "def new_dataset(ratings):\n",
    "    users = set(r.user_id for r in ratings)\n",
    "    items = set(r.item_id for r in ratings)\n",
    "    return Dataset(users, items, ratings)\n",
    "\n",
    "\n",
    "small_dataset = new_dataset([Rating(x['user_id'], x['movie_id'], x['rating']) for i, x in ratings.iterrows()])\n",
    "\n",
    "print('Dataset\\n\\n{}'.format(small_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def split_randomly(dataset, train_ratio=0.80):\n",
    "    ratings = dataset.ratings\n",
    "    shuffle(ratings)\n",
    "    size = int(len(ratings) * train_ratio)\n",
    "    train_ratings = ratings[:size]\n",
    "    test_ratings = ratings[size:]\n",
    "    return new_dataset(train_ratings), \\\n",
    "            new_dataset(test_ratings)\n",
    "\n",
    "train_valid_data, test_data = split_randomly(small_dataset)\n",
    "train_data, valid_data = split_randomly(train_valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for train: 640,133\n"
     ]
    }
   ],
   "source": [
    "train_eval = list(((r.user_id, r.item_id), r.rating) for r in train_data.ratings)\n",
    "print('Evaluation ratings for train: {:,d}'.format(len(train_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in train and validation: 3,392\n"
     ]
    }
   ],
   "source": [
    "# only items in train will be available for validation\n",
    "valid_items = train_data.items & valid_data.items\n",
    "print('Items in train and validation: {:,d}'.format(len(valid_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in validation with train items: 6,032\n"
     ]
    }
   ],
   "source": [
    "# users from validation that has any item from train\n",
    "valid_users = set(r.user_id for r in valid_data.ratings if r.item_id in train_data.items)\n",
    "print('Users in validation with train items: {:,d}'.format(len(valid_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in train and validation: 6,032\n"
     ]
    }
   ],
   "source": [
    "# only users in train are available for validation\n",
    "valid_users &= train_data.users\n",
    "print('Users in train and validation: {:,d}'.format(len(valid_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation ratings for validation: 160,004\n"
     ]
    }
   ],
   "source": [
    "valid_eval = valid_data.filter_ratings(valid_users, valid_items)\n",
    "print('Evaluation ratings for validation: {:,d}'.format(len(valid_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map User <-> index\n",
    "# Map Item <-> index\n",
    "IndexMapping = collections.namedtuple('IndexMapping', ['users_to_idx',\n",
    "                                                       'users_from_idx',\n",
    "                                                       'items_to_idx',\n",
    "                                                       'items_from_idx'])\n",
    "\n",
    "def map_index(values):\n",
    "    values_from_idx = dict(enumerate(values))\n",
    "    values_to_idx = dict((value, idx) for idx, value in values_from_idx.items())\n",
    "    return values_to_idx, values_from_idx\n",
    "\n",
    "def new_mapping(dataset):\n",
    "    users_to_idx, users_from_idx = map_index(dataset.users)\n",
    "    items_to_idx, items_from_idx = map_index(dataset.items)\n",
    "    return IndexMapping(users_to_idx, users_from_idx, items_to_idx, items_from_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.557\n",
      "RMSE: 0.214\n",
      "RMSE: 0.194\n",
      "RMSE: 0.190\n",
      "RMSE: 0.188\n",
      "RMSE: 0.187\n",
      "RMSE: 0.186\n",
      "RMSE: 0.185\n",
      "RMSE: 0.185\n",
      "RMSE: 0.184\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.factorization import WALSModel\n",
    "\n",
    "class ALSRecommenderModel:\n",
    "    \n",
    "    def __init__(self, user_factors, item_factors, mapping):\n",
    "        self.user_factors = user_factors\n",
    "        self.item_factors = item_factors\n",
    "        self.mapping = mapping\n",
    "    \n",
    "    def transform(self, x):\n",
    "        for user_id, item_id in x:\n",
    "            if user_id not in self.mapping.users_to_idx \\\n",
    "                or item_id not in self.mapping.items_to_idx:\n",
    "                yield (user_id, item_id), 0.0\n",
    "                continue\n",
    "            i = self.mapping.users_to_idx[user_id]\n",
    "            j = self.mapping.items_to_idx[item_id]\n",
    "            u = self.user_factors[i]\n",
    "            v = self.item_factors[j]\n",
    "            r = np.dot(u, v)\n",
    "            yield (user_id, item_id), r\n",
    "    \n",
    "    def recommend(self, user_id, num_items=10, items_exclude=set()):\n",
    "        i = self.mapping.users_to_idx[user_id]\n",
    "        u = self.user_factors[i]\n",
    "        V = self.item_factors\n",
    "        P = np.dot(V, u)\n",
    "        rank = sorted(enumerate(P), key=lambda p: p[1], reverse=True)\n",
    "\n",
    "        top = list()\n",
    "        k = 0\n",
    "        while k < len(rank) and len(top) < num_items:\n",
    "            j, r = rank[k]\n",
    "            k += 1\n",
    "\n",
    "            item_id = self.mapping.items_from_idx[j]\n",
    "            if item_id in items_exclude:\n",
    "                continue\n",
    "\n",
    "            top.append((item_id, r))\n",
    "\n",
    "        return top        \n",
    "    \n",
    "class ALSRecommender:\n",
    "    \n",
    "    def __init__(self, num_factors=10, num_iters=10, reg=1e-1):\n",
    "        self.num_factors = num_factors\n",
    "        self.num_iters = num_iters\n",
    "        self.regularization = reg\n",
    "\n",
    "    def fit(self, dataset, verbose=False):\n",
    "        with tf.Graph().as_default(), tf.Session() as sess:\n",
    "            input_matrix, mapping = self.sparse_input(dataset)\n",
    "            model = self.als_model(dataset)\n",
    "            self.train(model, input_matrix, verbose)\n",
    "            row_factor = model.row_factors[0].eval()\n",
    "            col_factor = model.col_factors[0].eval()\n",
    "            return ALSRecommenderModel(row_factor, col_factor, mapping)\n",
    "\n",
    "    def sparse_input(self, dataset):\n",
    "        mapping = new_mapping(dataset)\n",
    "\n",
    "        indices = [(mapping.users_to_idx[r.user_id],\n",
    "                    mapping.items_to_idx[r.item_id])\n",
    "                   for r in dataset.ratings]\n",
    "        values = [r.rating for r in dataset.ratings]\n",
    "        shape = (dataset.n_users, dataset.n_items)\n",
    "\n",
    "        return tf.SparseTensor(indices, values, shape), mapping\n",
    "    \n",
    "    def als_model(self, dataset):\n",
    "        return WALSModel(\n",
    "            dataset.n_users,\n",
    "            dataset.n_items,\n",
    "            self.num_factors,\n",
    "            regularization=self.regularization,\n",
    "            unobserved_weight=0)\n",
    "\n",
    "    def train(self, model, input_matrix, verbose=False):\n",
    "        rmse_op = self.rmse_op(model, input_matrix) if verbose else None\n",
    "\n",
    "        row_update_op = model.update_row_factors(sp_input=input_matrix)[1]\n",
    "        col_update_op = model.update_col_factors(sp_input=input_matrix)[1]\n",
    "\n",
    "        model.initialize_op.run()\n",
    "        model.worker_init.run()\n",
    "        for _ in range(self.num_iters):\n",
    "            # Update Users\n",
    "            model.row_update_prep_gramian_op.run()\n",
    "            model.initialize_row_update_op.run()\n",
    "            row_update_op.run()\n",
    "            # Update Items\n",
    "            model.col_update_prep_gramian_op.run()\n",
    "            model.initialize_col_update_op.run()\n",
    "            col_update_op.run()\n",
    "\n",
    "            if verbose:\n",
    "                print('RMSE: {:,.3f}'.format(rmse_op.eval()))\n",
    "\n",
    "    def approx_sparse(self, model, indices, shape):\n",
    "        row_factors = tf.nn.embedding_lookup(\n",
    "            model.row_factors,\n",
    "            tf.range(model._input_rows),\n",
    "            partition_strategy=\"div\")\n",
    "        col_factors = tf.nn.embedding_lookup(\n",
    "            model.col_factors,\n",
    "            tf.range(model._input_cols),\n",
    "            partition_strategy=\"div\")\n",
    "\n",
    "        row_indices, col_indices = tf.split(indices,\n",
    "                                            axis=1,\n",
    "                                            num_or_size_splits=2)\n",
    "        gathered_row_factors = tf.gather(row_factors, row_indices)\n",
    "        gathered_col_factors = tf.gather(col_factors, col_indices)\n",
    "        approx_vals = tf.squeeze(tf.matmul(gathered_row_factors,\n",
    "                                           gathered_col_factors,\n",
    "                                           adjoint_b=True))\n",
    "\n",
    "        return tf.SparseTensor(indices=indices,\n",
    "                               values=approx_vals,\n",
    "                               dense_shape=shape)\n",
    "\n",
    "    def rmse_op(self, model, input_matrix):\n",
    "        approx_matrix = self.approx_sparse(model, input_matrix.indices, input_matrix.dense_shape)\n",
    "        err = tf.sparse_add(input_matrix, approx_matrix * (-1))\n",
    "        err2 = tf.square(err)\n",
    "        n = input_matrix.values.shape[0].value\n",
    "        return tf.sqrt(tf.sparse_reduce_sum(err2) / n)\n",
    "\n",
    "\n",
    "als = ALSRecommender()\n",
    "als_model = als.fit(train_data, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4277.0 2711.0 1.0 0.6988603\n",
      "4740.0 2513.0 0.75 0.59964603\n",
      "2119.0 2688.0 0.5 0.58921945\n",
      "308.0 2144.0 0.75 0.6795177\n",
      "3513.0 1917.0 0.5 0.21331201\n",
      "1447.0 3863.0 1.0 0.7512944\n",
      "4732.0 2141.0 0.5 0.6447932\n",
      "5840.0 2268.0 1.0 0.7557846\n",
      "3911.0 3653.0 1.0 0.5332837\n",
      "3091.0 608.0 0.75 0.7642213\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    x, y  = valid_eval[k]\n",
    "    _,  y_hat = list(als_model.transform([x]))[0]\n",
    "    print(*x, y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (train): 0.184\n",
      "RMSE (validation): 0.231\n"
     ]
    }
   ],
   "source": [
    "def _rmse(model, data):\n",
    "    x, y = zip(*data)\n",
    "    y_hat = list(r_hat for _, r_hat in model.transform(x))\n",
    "    return np.sqrt(np.mean(np.square(np.subtract(y, y_hat))))\n",
    "\n",
    "def eval_rmse(model):\n",
    "    rmse = _rmse(model, train_eval)\n",
    "    print('RMSE (train): {:,.3f}'.format(rmse))\n",
    "    \n",
    "    rmse = _rmse(model, valid_eval)\n",
    "    print('RMSE (validation): {:,.3f}'.format(rmse))\n",
    "\n",
    "eval_rmse(als_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "RMSE: 0.559\n",
      "RMSE: 0.218\n",
      "RMSE: 0.197\n",
      "RMSE: 0.191\n",
      "RMSE: 0.189\n",
      "RMSE: 0.188\n",
      "RMSE: 0.187\n",
      "RMSE: 0.186\n",
      "RMSE: 0.185\n",
      "RMSE: 0.185\n",
      "\n",
      "Evaluation...\n",
      "\n",
      "RMSE (train): 0.185\n",
      "RMSE (validation): 0.231\n"
     ]
    }
   ],
   "source": [
    "als = ALSRecommender(num_factors=10, num_iters=10, reg=0.1)\n",
    "print('Training...\\n')\n",
    "als_model = als.fit(train_data, verbose=True)\n",
    "print('\\nEvaluation...\\n')\n",
    "eval_rmse(als_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
